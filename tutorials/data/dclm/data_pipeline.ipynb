{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327f594a",
   "metadata": {},
   "source": [
    "# Data Processing for Megatron Bridge LLMs with the DCLM Dataset\n",
    "\n",
    "This notebook provides a step-by-step guide to preprocessing a DCLM subdataset for use with Megatron Bridge. In this example, the subdataset is 68 GB in its compressed form and expands to 199 GB once decompressed. For more information about the dataset, check out the [README](https://github.com/NVIDIA-NeMo/Megatron-Bridge/blob/main/tutorials/data/dclm/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0d0ed",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "The example below demonstrates how to download the `global-shard_01_of_10/local-shard_0_of_10` subdataset of DCLM dataset from HuggingFace. To download the full dataset, set `allow_patterns` to `*.jsonl.zst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "# login to HF using token\n",
    "login(token=\"input your HF token\")\n",
    "\n",
    "# download dataset\n",
    "snapshot_download(\n",
    "    repo_id=\"mlfoundations/dclm-baseline-1.0\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"/data/dclm\",\n",
    "    allow_patterns=\"global-shard_01_of_10/local-shard_0_of_10/**\", # set to '*.jsonl.zst' to download full dataset\n",
    "    resume_download=True,\n",
    "    max_workers=32,  # Don't hesitate to increase this number to lower the download time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67bb66",
   "metadata": {},
   "source": [
    "## Decompress files\n",
    "\n",
    "The dataset is hosted on HuggingFace in compressed format. To work with the `.jsonl` files, the downloaded files need to be decompressed first.\n",
    "\n",
    "**Note:** This script may require the `parallel` and `zstd` packages to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# decompress files\n",
    "PATH_TO_SAVE=/data/dclm/decompressed\n",
    "SOURCE_DIR=/data/dclm/global-shard_01_of_10/local-shard_0_of_10\n",
    "NUM_WORKERS=32\n",
    "\n",
    "mkdir -p ${PATH_TO_SAVE}\n",
    "cd ${SOURCE_DIR}\n",
    "find . -name \"*.zst\" | parallel -j${NUM_WORKERS} \"zstd -d {} -o ${PATH_TO_SAVE}/{.}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d038c59",
   "metadata": {},
   "source": [
    "## Merge files\n",
    "\n",
    "Each DCLM subdataset contains hundreds of small `.jsonl` files. To simplify handling, we merge all `.jsonl` files from the current subdataset into a single `.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ab800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# merge files\n",
    "SOURCE_DIR=/data/dclm/decompressed\n",
    "PATH_TO_SAVE=/data/dclm/decompressed/merged.jsonl\n",
    "\n",
    "cd ${SOURCE_DIR}\n",
    "awk '1' *.jsonl > ${PATH_TO_SAVE}\n",
    "\n",
    "# remove small .jsonl files\n",
    "rm shard_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012dbe2",
   "metadata": {},
   "source": [
    "# Shuffle data\n",
    "\n",
    "This example demonstrates how to shuffle the data. First, we split the merged `.jsonl` file into smaller chunks to parallelize shuffling and speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# split file into chunks before shuffling\n",
    "LINES_PER_SPLIT=1000000\n",
    "SOURCE_FILE=/data/dclm/decompressed/merged.jsonl\n",
    "CHUNKS_DIR=/data/dclm/decompressed/chunks\n",
    "\n",
    "mkdir -p ${CHUNKS_DIR}\n",
    "split -l ${LINES_PER_SPLIT} ${SOURCE_FILE} ${CHUNKS_DIR}/chunk_\n",
    "\n",
    "# shuffle files\n",
    "NUM_WORKERS=16\n",
    "SHUFFLE_CHUNKS_DIR=/data/dclm/decompressed/shuffled_chunks\n",
    "PATH_TO_SAVE=/data/dclm/decompressed/shuffled.jsonl\n",
    "\n",
    "mkdir -p ${SHUFFLE_CHUNKS_DIR}\n",
    "ls \"${CHUNKS_DIR}\"/chunk_* | parallel -j\"${NUM_WORKERS}\" 'shuf {} -o '\"${SHUFFLE_CHUNKS_DIR}\"'/$(basename {})_shuf'\n",
    "\n",
    "# remove unshuffled chunks\n",
    "rm -rf ${CHUNKS_DIR}\n",
    "# merge shuffled chunnks into single .jsonl file\n",
    "awk '1' ${SHUFFLE_CHUNKS_DIR}/chunk_* > ${PATH_TO_SAVE}\n",
    "# remove shuffled chunks\n",
    "rm -rf ${SHUFFLE_CHUNKS_DIR}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae67b7",
   "metadata": {},
   "source": [
    "## Preprocess Data to bin/idx format\n",
    "\n",
    "This step uses the data preprocessing [script](https://github.com/NVIDIA/Megatron-LM/blob/main/tools/preprocess_data.py) from Megatron-LM, so Megatron-LM must be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# install Megatron-LM\n",
    "\n",
    "# Install Megatron Core with required dependencies\n",
    "pip install megatron-core\n",
    "pip install --no-build-isolation transformer-engine[pytorch]\n",
    "\n",
    "# Clone repository for examples\n",
    "git clone https://github.com/NVIDIA/Megatron-LM.git\n",
    "cd Megatron-LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabf8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# run data preprocessing\n",
    "python3 Megatron-LM/tools/preprocess_data.py \\\n",
    "    --input /data/dclm/decompressed/shuffled.jsonl \\\n",
    "    --output-prefix /data/dclm/preprocessed \\\n",
    "    --tokenizer-type HuggingFaceTokenizer \\\n",
    "    --tokenizer-model meta-llama/Meta-Llama-3-8B \\\n",
    "    --log-interval 10000 \\\n",
    "    --workers 32 \\\n",
    "    --append-eod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
